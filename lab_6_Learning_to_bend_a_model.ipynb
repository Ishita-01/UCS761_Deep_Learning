{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eQ6NVV83NUHa"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "lB6GGpUkPKyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.linspace(0, 10, 50).reshape(-1, 1)\n",
        "noise = np.random.normal(0, 0.2, size=(50, 1))\n",
        "y= np.log(X + 1) + noise\n"
      ],
      "metadata": {
        "id": "537L0uqYPJ7x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Number of hidden units = 3\n",
        "- More than 1 allows bending (non-linearity)\n",
        "- Too many would cause instability and overfitting"
      ],
      "metadata": {
        "id": "3JD1vLoiPmg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Initialization"
      ],
      "metadata": {
        "id": "ohgoz2iVPx9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input to hidden\n",
        "W1 =np.random.uniform(-1, 1, size=(1, 3))\n",
        "b1 =np.zeros((1, 3))\n",
        "\n",
        "# Hidden to Output\n",
        "W2= np.random.uniform(-1, 1, size=(3, 1))\n",
        "b2= np.zeros((1, 1))"
      ],
      "metadata": {
        "id": "FBXuKU9qPwUT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation function"
      ],
      "metadata": {
        "id": "tHiN5esQQLOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def activation_slope(z):\n",
        "    return (z > 0).astype(float)\n"
      ],
      "metadata": {
        "id": "8qhem_nZQKR1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "X9Rkz05rQgdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Forward Pass\n",
        "    z1 = X @ W1 + b1\n",
        "    h = activation(z1)\n",
        "    y_hat = h @ W2 + b2\n",
        "\n",
        "    # Loss\n",
        "    error = y_hat - y\n",
        "    loss = np.mean(error ** 2)\n",
        "\n",
        "    # Backward Pass\n",
        "    dL_dy = 2 * error/len(X)\n",
        "\n",
        "    dL_dW2 = h.T @ dL_dy\n",
        "    dL_db2 = np.sum(dL_dy, axis=0, keepdims=True)\n",
        "\n",
        "    dL_dh = dL_dy @ W2.T\n",
        "    dL_dz1 = dL_dh * activation_slope(z1)\n",
        "\n",
        "    dL_dW1 = X.T @ dL_dz1\n",
        "    dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)\n",
        "\n",
        "    #  Update Parameter\n",
        "    W1 -= learning_rate * dL_dW1\n",
        "    b1 -= learning_rate * dL_db1\n",
        "    W2 -= learning_rate * dL_dW2\n",
        "    b2 -= learning_rate * dL_db2\n",
        "\n",
        "    # Monitor\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bsggawjQRwK",
        "outputId": "71a07627-3ef7-4dcd-8404-5fbb97c9ca60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 6.3798\n",
            "Epoch 100, Loss: 0.1025\n",
            "Epoch 200, Loss: 0.0769\n",
            "Epoch 300, Loss: 0.0681\n",
            "Epoch 400, Loss: 0.0651\n",
            "Epoch 500, Loss: 0.0640\n",
            "Epoch 600, Loss: 0.0636\n",
            "Epoch 700, Loss: 0.0635\n",
            "Epoch 800, Loss: 0.0634\n",
            "Epoch 900, Loss: 0.0634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining complete \")\n",
        "print(\"Final Loss:\", loss)\n",
        "\n",
        "print(\"\\n Sample predictions:\")\n",
        "for i in range(5):\n",
        "    print(f\"X: {X[i][0]:.2f} | True: {y[i][0]:.4f} | Pred: {y_hat[i][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7beBnsdRKR9",
        "outputId": "e468884b-5b6c-43b5-fdf2-993cb5f4be55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete \n",
            "Final Loss: 0.06336819001043968\n",
            "\n",
            " Sample predictions:\n",
            "X: 0.00 | True: -0.1242 | Pred: 0.7185\n",
            "X: 0.20 | True: 0.3201 | Pred: 0.7636\n",
            "X: 0.41 | True: 0.5658 | Pred: 0.8033\n",
            "X: 0.61 | True: 0.6376 | Pred: 0.8430\n",
            "X: 0.82 | True: 1.0340 | Pred: 0.8827\n"
          ]
        }
      ]
    }
  ]
}